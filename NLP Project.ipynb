{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khomkin Konstantin, cohort 53DS\n",
    "Machine Learning for Texts Project, v.2.0 23.02.2023\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Wikishop online store launches a new service. Now users can edit and supplement product descriptions like in wiki communities. That is, customers offer their edits and comment on the changes of others. The store needs a tool that will search for toxic comments and send them for moderation. \n",
    "\n",
    "Let's train the model to categorize comments into positive and negative. We have at our disposal a dataset with markup on the toxicity of edits.\n",
    "Let's build a model with a target value of quality metric F1 not less than 0.75. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "import re \n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gap\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the try-except construct to load the file both for the local version and for working from the simulator\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('toxic_comments.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"Unnamed: 0 \" does not contain useful information, so we delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dictionary\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a simple lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for line-by-line processing (regularization, tokenization and lemmatize)\n",
    "\n",
    "def lemmatize(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text) # regularize \n",
    "    list = SpaceTokenizer().tokenize(text) # tokenize by separating with spaces\n",
    "    res = []\n",
    "    \n",
    "    for i in range(0, len(list), 1):\n",
    "        res.append(wnl.lemmatize(list[i])) # lemmatize each word\n",
    "    \n",
    "    return \" \".join(res) #  return glued words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the \"text\" column, using a lambda function, collect the result in \"lemm_text\"\n",
    "df['lemm_text'] = df['text'].apply ( lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww  He match this background colour I m see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man  I m really not trying to edit war  It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You  sir  are my hero  Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D aww  He match this background colour I m see...  \n",
       "2  Hey man  I m really not trying to edit war  It...  \n",
       "3    More I can t make any real suggestion on imp...  \n",
       "4  You  sir  are my hero  Any chance you remember...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect corpus of words from lemmatized column\n",
    "corpus = df['lemm_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features are collected in the corpus, targets in the \"toxic\" column\n",
    "\n",
    "#v2\n",
    "features = corpus\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divided into training and test samples, 4:1 ratio\n",
    "\n",
    "#v2\n",
    "corpus_train, corpus_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target, random_state=12345) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы tf_idf_train: (127433, 144134)\n",
      "Размер матрицы tf_idf_test: (31859, 144134)\n"
     ]
    }
   ],
   "source": [
    "# Build a TF-IDF word importance score matrix\n",
    "\n",
    "# v2\n",
    "count_tf_idf = TfidfVectorizer() \n",
    "\n",
    "# Vectorizer is trained on a training sample\n",
    "count_tf_idf.fit(corpus_train) \n",
    "\n",
    "# Vectorize the training and test samples\n",
    "tf_idf_train = count_tf_idf.transform(corpus_train) \n",
    "\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test) \n",
    "\n",
    "print(\"Размер матрицы tf_idf_train:\", tf_idf_train.shape)\n",
    "print(\"Размер матрицы tf_idf_test:\", tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save matrices of vectorized words as features\n",
    "features_train = tf_idf_train\n",
    "features_test = tf_idf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter dictionaries where we will store models, their metrics and hyperparameters\n",
    "models = {}\n",
    "models_scores = {}\n",
    "param_search = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the models\n",
    "models[0] = LogisticRegression(class_weight='balanced', random_state=12345) \n",
    "models[1] = DecisionTreeClassifier(class_weight='balanced', random_state=12345)\n",
    "models[2] = RandomForestClassifier(class_weight='balanced', random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hyperparameter sets for selection\n",
    "param_search[0] = {'solver' : ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']}\n",
    "param_search[1] = {'max_depth' : [25, 50]}\n",
    "param_search[2] = {'n_estimators' : [5, 15], 'max_depth' : [25, 50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель:  LogisticRegression(class_weight='balanced', random_state=12345)\n",
      "Лучшие гиперпараметры: {'solver': 'saga'}\n",
      "F1: 0.75\n",
      "\n",
      "Модель:  DecisionTreeClassifier(class_weight='balanced', random_state=12345)\n",
      "Лучшие гиперпараметры: {'max_depth': 50}\n",
      "F1: 0.56\n",
      "\n",
      "Модель:  RandomForestClassifier(class_weight='balanced', random_state=12345)\n",
      "Лучшие гиперпараметры: {'max_depth': 50, 'n_estimators': 15}\n",
      "F1: 0.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we loop through all models and determine the best model using GridSearchCV\n",
    "for i in range(len(models)):\n",
    "    gsearch = GridSearchCV(estimator=models[i], \n",
    "                           cv=10,\n",
    "                           param_grid=param_search[i],\n",
    "                           scoring='f1')\n",
    "    model_grid = gsearch.fit(features_train, target_train)\n",
    "    models_scores[i] =   abs(model_grid.best_score_)\n",
    "    print('Модель: ', models[i])\n",
    "    print('Лучшие гиперпараметры: '+str(model_grid.best_params_))\n",
    "    print(f'F1: {models_scores[i]:.2f}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая модель: LogisticRegression(class_weight='balanced', random_state=12345)\n",
      "Метрика F1= 0.75\n"
     ]
    }
   ],
   "source": [
    "# display the best model and hyperparameter information on the screen\n",
    "print('Лучшая модель:',models[max(models_scores, key=models_scores.get)])\n",
    "print(f'Метрика F1= {models_scores[max(models_scores, key=models_scores.get)]:.2}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a prediction on the test sample, applying the best model, and estimate the f1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранная модель:  LogisticRegression(class_weight='balanced', random_state=12345, solver='saga')\n",
      "Итоговая метрика f1 на тестовой выборке: 0.75\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight='balanced', solver='saga', random_state=12345)\n",
    "               \n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "predict = model.predict(features_test)\n",
    "\n",
    "score = f1_score(target_test, predict)\n",
    "\n",
    "print('Выбранная модель: ', model)\n",
    "print(f'Итоговая метрика f1 на тестовой выборке: {score:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions:\n",
    "1. preprocessing of the original data was carried out, the uninformative column \"Unnamed :0\" was removed\n",
    "2. Regularization, tokenization and lemmatization of the original tweets were performed, for which a function for line-by-line processing was defined\n",
    "3. lemmatized text is added to the original array, a corpus of words is collected\n",
    "4. The importance of words is evaluated, the evaluation matrix is collected, which is a feature matrix for model training.\n",
    "5. The target feature is the \"toxic\" column.\n",
    "6. 4 model classifiers are announced: \n",
    "<br>6.1. logistic regression\n",
    "<br>6.2 Logistic regression with solver found by GridSearch tool 6.3.\n",
    "<br>6.3. decision tree\n",
    "<br>6.4. Random Forest\n",
    "7. The original sample is divided into training and test samples in the ratio of 80:20.\n",
    "8. GridSearch tool with crossvalidation searched for the best hyperparameters with comparison by f1 metric.\n",
    "9. The best metric was found for the simple logistic regression model with Saga solver, f1 = 0.75.\n",
    "10. The best model is applied for prediction on the test sample, the final metric is f1 = 0.75.\n",
    "\n",
    "Unfortunately, we failed to apply BERT in the project (locally we also lacked resources), the pymystem3 library did not work locally, and the kernel \"crashed\" in the simulator when lemmatizing 150K lines of the task. Therefore, we used \"simple\" tokenization and lemmatization tools from the nltk library\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
