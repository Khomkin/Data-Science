{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет! меня зовут Люман Аблаев. Сегодня я проверю твой проект.\n",
    "<br> Дальнейшее общение будет происходить на \"ты\" если это не вызывает никаких проблем.\n",
    "<br> Желательно реагировать на каждый мой комментарий ('исправил', 'не понятно как исправить ошибку', ...)\n",
    "<br> Пожалуйста, не удаляй комментарии ревьюера, так как они повышают качество повторного ревью.\n",
    "\n",
    "Комментарии будут в <font color='green'>зеленой</font>, <font color='blue'>синей</font> или <font color='red'>красной</font> рамках:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Если все сделано отлично\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Если можно немного улучшить\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b> Если требуются исправления. Работа не может быть принята с красными комментариями.\n",
    "</div>\n",
    "\n",
    "-------------------\n",
    "\n",
    "Будет очень хорошо, если ты будешь помечать свои действия следующим образом:\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Комментарий студента:</b> ..\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения ...\n",
    "</div>\n",
    "\n",
    "<font color='orange' style='font-size:24px; font-weight:bold'>Общее впечатление</font>\n",
    "* Спасибо за  качественную работу, мне она понравилась!\n",
    "- Работа очень лаконичная, было приятно ее читать.\n",
    "- Я оставил некоторые советы надеюсь они будут полезными или интересными.\n",
    "- Есть маленький недочет, но ты его поправишь за минуту =)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='orange' style='font-size:24px; font-weight:bold'>Общее впечатление[2]</font>\n",
    "* Спасибо за оперативность\n",
    "- Приятно видеть фидбек на комментарии.\n",
    "- Недочеты исправлены - работа полностью корректна\n",
    "- Рад был быть полезен. Удачи в дальнейшем обучении!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Хомкин Константин, когорта 53DS\n",
    "Проект \"Машинное обучение в бизнесе\", v.2.0 15.12.2022\n",
    "\n",
    "# Введение\n",
    "Мы работаем в добывающей компании «ГлавРосГосНефть». Необходимо на основе анализа имеющейся  информации принять решение о том, где бурить новую скважину.\n",
    "\n",
    "Для выбора новой локации необходимо:\n",
    "Собрать по выбранномым регионам основные характеристики скважин: качество нефти и объём её запасов;\n",
    "Построить модель предсказания объёма запасов в новых скважинах;\n",
    "Выбрать скважины с самыми высокими оценками значений;\n",
    "Определить регион с максимальной суммарной прибылью отобранных скважин.\n",
    "\n",
    "Для анализа нам доступны данные о пробах нефти в трёх регионах и характеристики для каждой скважины. \n",
    "\n",
    "В проекте построим модель для определения региона, где добыча принесёт наибольшую прибыль. Проанализируем возможную прибыль и риски техникой Bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 1. Импортируем необходимые библиотеки, откроем и изучим структуру доступных файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # библиотека Pandas нам нужна для работы с датасетом\n",
    "\n",
    "from sklearn.model_selection import train_test_split # функция train_test_split нужна для разделения данных на выборки\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # метод стандартизации масштаба данных\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # модель \"Линейная регрессия\"\n",
    "\n",
    "from sklearn.metrics import mean_squared_error # метод для расчета среднеквадратичной ошибки\n",
    "\n",
    "import numpy as np # библиотека Numpy нужна для быстрого поиска максимальных значений\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Импорты  на месте\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Поскольку у нас три идентичных датасета, необходимо трижды повторять все манипуляции с каждым датасетом. При этом количество датасетов \"на входе\" в проект может измениться. Поэтому, для 1) оптимизации длины кода и 2) универсализации кода для разного количества данных на входе объединим все данные в один объект - словарь. Изменяя лишь переменную с номерами файлов мы сможет масштабировать проект до любого адекватного количества данных о скважинах.**\n",
    "\n",
    "**При этом, с точки зрения абсолютой длины кода, также всё нормально: даже если какая-то операция с датасетом выполняется \"в одну строку\", например извелечение признаков, значит для трех датасетов нужно повторить эту строку трижды, тогда как с использованием словаря и цикла мы \"умещаемся\" всего в две строки, что и будет видно ниже в коде.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Отличное решение\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявляем переменную, где будем хранить уникальные признаки разных датасетов, в данном случае - номера файлов\n",
    "regions = [0, 1, 2]  \n",
    "\n",
    "# датасеты \"сложим\" в один словарь и прочитаем файлы одной строкой с внутренним циклом\n",
    "df = {reg:pd.read_csv('geo_data_'+str(reg)+'.csv') for reg in regions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id        f0        f1        f2     product\n",
      "0  txEyH  0.705745 -0.497823  1.221170  105.280062\n",
      "1  2acmU  1.334711 -0.340164  4.365080   73.037750\n",
      "2  409Wp  1.022732  0.151990  1.419926   85.265647\n",
      "3  iJLyR -0.032172  0.139033  2.978566  168.620776\n",
      "4  Xdl7t  1.988431  0.155413  4.751769  154.036647\n",
      "      id         f0         f1        f2     product\n",
      "0  kBEdx -15.001348  -8.276000 -0.005876    3.179103\n",
      "1  62mP7  14.272088  -3.475083  0.999183   26.953261\n",
      "2  vyE1P   6.263187  -5.948386  5.001160  134.766305\n",
      "3  KcrkZ -13.081196 -11.506057  4.999415  137.945408\n",
      "4  AHL4O  12.702195  -8.147433  5.004363  134.766305\n",
      "      id        f0        f1        f2     product\n",
      "0  fwXo0 -1.146987  0.963328 -0.828965   27.758673\n",
      "1  WJtFt  0.262778  0.269839 -2.530187   56.069697\n",
      "2  ovLUW  0.194587  0.289035 -5.586433   62.871910\n",
      "3  q6cA6  2.236060 -0.553760  0.930038  114.572842\n",
      "4  WPMUX -0.515993  1.716266  5.899011  149.600746\n"
     ]
    }
   ],
   "source": [
    "# выводим первые 5 строк каждого датасета\n",
    "for i in range(len(regions)):\n",
    "    print(df[i].head(5)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "данные выглядят структурированными, потребуется удаление неинформативного столбца с id скважины, а также масштабирование признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, f0, f1, f2, product]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [id, f0, f1, f2, product]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [id, f0, f1, f2, product]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Проверим данные на дубликаты\n",
    "for i in range(len(regions)):\n",
    "    print(df[i][df[i].duplicated()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полных дубликатов нет. Однако, стоит проверить отдельно по столбцу id, возможно есть записи под одинаковыми именами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дубликатов по id в регионе 0   : 10\n",
      "Дубликатов по id в регионе 1   : 4\n",
      "Дубликатов по id в регионе 2   : 4\n"
     ]
    }
   ],
   "source": [
    "# Выведем на экран разницу общего кол-ва строк и количества уникальных id\n",
    "for i in range(len(regions)):\n",
    "    print('Дубликатов по id в регионе', i, '  :', pd.Series(df[i]['id']).count()-pd.Series(df[i]['id'].unique()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бинго! Есть под одинаковыми id по несколько записей. При этом это не полные дубликаты, то есть данные в остальных (кроме id) столбцах отличаются. Какие именно строки истинные - уже не выяснить, поэтому для снижения ошибок моделей данные под совпадающими id из датафрейма удалим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим все дубликаты по id, не сохраняя никакую запись (аргумент keep=False)\n",
    "for i in range(len(regions)): \n",
    "    df[i] = df[i].drop_duplicates(subset='id', keep=False, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Дубликатов по id в регионе 0   : 0\n",
      "Дубликатов по id в регионе 1   : 0\n",
      "Дубликатов по id в регионе 2   : 0\n"
     ]
    }
   ],
   "source": [
    "# Еще раз выведем на экран разницу общего кол-ва строк и количества уникальных id\n",
    "\n",
    "for i in range(len(regions)):\n",
    "    print('Дубликатов по id в регионе', i, '  :', pd.Series(df[i]['id']).count()-pd.Series(df[i]['id'].unique()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликатов не осталось. Теперь несущественый для модели признак можно удалить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем несущественный признак\n",
    "for i in range(len(regions)):\n",
    "    df[i]= df[i].drop(['id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несущественные данные удалены. Проверим таблицы на полноту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99980 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      "f0         99980 non-null float64\n",
      "f1         99980 non-null float64\n",
      "f2         99980 non-null float64\n",
      "product    99980 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 3.8 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99992 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      "f0         99992 non-null float64\n",
      "f1         99992 non-null float64\n",
      "f2         99992 non-null float64\n",
      "product    99992 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 3.8 MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99992 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      "f0         99992 non-null float64\n",
      "f1         99992 non-null float64\n",
      "f2         99992 non-null float64\n",
      "product    99992 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 3.8 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(regions)):\n",
    "    print(df[i].info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               f0        f1        f2   product\n",
      "f0       1.000000 -0.440724 -0.003204  0.143504\n",
      "f1      -0.440724  1.000000  0.001783 -0.192338\n",
      "f2      -0.003204  0.001783  1.000000  0.483628\n",
      "product  0.143504 -0.192338  0.483628  1.000000\n",
      "               f0        f1        f2   product\n",
      "f0       1.000000  0.182263 -0.001821 -0.030534\n",
      "f1       0.182263  1.000000 -0.002608 -0.010167\n",
      "f2      -0.001821 -0.002608  1.000000  0.999397\n",
      "product -0.030534 -0.010167  0.999397  1.000000\n",
      "               f0        f1        f2   product\n",
      "f0       1.000000  0.000501 -0.000454 -0.001974\n",
      "f1       0.000501  1.000000  0.000763 -0.001046\n",
      "f2      -0.000454  0.000763  1.000000  0.445873\n",
      "product -0.001974 -0.001046  0.445873  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Поскольку планируем использовать линейные модели, посмотрим на корреляции признаков:\n",
    "\n",
    "for i in range(len(regions)):\n",
    "    print(df[i].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения \n",
    "\n",
    "Изучеы корреляции признаков. По региону \"2\" всё отлично, корреляции минимальные. В регионе \"0\" есть заметная отрицательная корреляция признаков f0 и f1 (-0.44), а также признака f2 и целевого признака product (0.48). \n",
    "А вот с датасетов региона \"1\" ситуация сложнее. Высокая корреляция признака f2 и целевого признака product, значение практически единица. Это говорит о том, что имеется сильно выраженная прямопропорциональная зависимость между признаком f2 и целевым признаком. По условиям проекта - все признаки значимы и удалять мы их не будем. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех[2]:</b> Есть контакт!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:** Данные загружены, проверены на полноту и сотоветствие типов, несущественные данные (id) удалены. Требуется масштабирование, которое будет проведено после раздедения данных на выборки, во избежание эффекта утечки данных.\n",
    "\n",
    "# Шаг 2. Обучение и проверка модели \n",
    "\n",
    "Разбьем данные на обучающую и валидационную выборки в соотношении 75:25.<br>\n",
    "Обучим модель и сделаем предсказания на валидационной выборке.<br>\n",
    "Сохраним предсказания и правильные ответы на валидационной выборке.<br>\n",
    "Рассчитаем средний запас предсказанного сырья и RMSE модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Данные осмотрены, но можно было бы еще как минимум изучить корреляции, так как мы планируем использовать линейные модели, а для них это важно. \n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет </b>  Советую обратить внимание на библиотеки sweetviz и pandas_profiling помогут в проведении более тщательного EDA анализа. Исследовательский анализ можно делать и с помощью ручного вызова функций дефолтных библиотек. Данные библиотеки хороши для максимизации комфорта презентации результатов анализа бизнес-пользователям.  \n",
    "Очень просты в использоовании, на вход кладется датафрейм: pandas_profiling.ProfileReport(df) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обявляем переменные (также словари) для хранения выборок\n",
    "features = {}\n",
    "target = {}\n",
    "\n",
    "# Делим датасеты на выборки и \"складываем\" в одноименные переменные \n",
    "for i in range(len(regions)):\n",
    "    features[i]= df[i].drop(['product'], axis=1) # извлекаем признаки датасета i\n",
    "    target[i] = df[i]['product'] # извлекаем целевой признак датасета i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обявляем переменные (также словари) для хранения обучающих и валидационных выборок\n",
    "features_train = {}\n",
    "features_valid = {}\n",
    "target_train = {}\n",
    "target_valid = {}\n",
    "\n",
    "\n",
    "# отделяем 25% данных для \"валидационной\" выборки \n",
    "for i in range(len(regions)):\n",
    "    features_train[i], features_valid[i], target_train[i], target_valid[i] = train_test_split(\n",
    "        features[i], target[i], test_size=0.25, random_state=12345) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проведем масштабирование признаков\n",
    "\n",
    "scaler = StandardScaler() # объявляем метод\n",
    "\n",
    "for i in range(len(regions)):\n",
    "    scaler.fit(features_train[i]) # Обучаем метод только на обучающей выборке\n",
    "    features_train[i] = pd.DataFrame(scaler.transform(features_train[i]), columns=features_train[i].columns) # Преобразуем данные и сохраняем в новой переменной в структуре DataFrame()\n",
    "    features_valid[i] = pd.DataFrame(scaler.transform(features_valid[i]), columns=features_valid[i].columns) # Преобразуем данные и сохраняем в новой переменной в структуре DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Регион 0: Средний запас сырья на скважину 92.42384/предс/, 92.39349/реал/, RMSE 37.71690 (тыс.баррелей)\n",
      "Регион 1: Средний запас сырья на скважину 68.98312/предс/, 68.98037/реал/, RMSE 0.89149 (тыс.баррелей)\n",
      "Регион 2: Средний запас сырья на скважину 95.11622/предс/, 94.54782/реал/, RMSE 39.97554 (тыс.баррелей)\n"
     ]
    }
   ],
   "source": [
    "# Обучим модель \"Линейная регрессия\" для каждого датасета отдельно\n",
    "\n",
    "# Объявляем необхожимые переменные\n",
    "model = {}\n",
    "predictions_valid = {}\n",
    "result = {}\n",
    "zapas_true = {}\n",
    "zapas_pred = {}\n",
    "\n",
    "# Инициализирем модель LinearRegression три раза так как обучать будем каждую на отдельной выборке\n",
    "for i in range(len(regions)):\n",
    "    model[i] = LinearRegression()\n",
    "\n",
    "# Обучаем каждую модель отдельно на своей выборке, рассчитываем предсказания и RMSE\n",
    "for i in range(len(regions)):\n",
    "    model[i].fit(features_train[i], target_train[i]) # обуаем модель на тренировочной выборке\n",
    "    predictions_valid[i] = model[i].predict(features_valid[i]) # получаем предсказания модели на валидационной выборке\n",
    "    result[i] = mean_squared_error(target_valid[i], predictions_valid[i])**0.5 # получаем значение метрики RMSE на валидационной выборке\n",
    "    zapas_pred[i] = predictions_valid[i].mean()  # расчет среднего запаса сырья по предсказанию\n",
    "    zapas_true[i] = target_valid[i].mean() # расчет реального среднего запаса сырья\n",
    "    print(f'Регион {i}: Средний запас сырья на скважину {zapas_pred[i]:.5f}/предс/, {zapas_true[i]:.5f}/реал/, RMSE {result[i]:.5f} (тыс.баррелей)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**<br>\n",
    "Метрика **RMSE** (средневкадратичное отклонение) исслюстрирует насколько хорошо модель соответствует набору данных. Чем больше RMSE, тем больше разница между прогнозируемыми и наблюдаемыми значениями, а это означает, что модель хуже соответствует данным. И наоборот, чем меньше RMSE, тем лучше модель соответствует данным. Таким образом мы видим, что в нашем проекте наилучшее предсказание получилось по данным региона 1, значение RMSE минимальное.  По регионам 0 и 2 среднеквадративное отклонение значительно выше, хотя средние значения предсказанных запасов сырья в целом близки к реальным данным.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Шаг проделан лаконично, все метрики посчитаны корректно\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 3. Подготовка к расчёту прибыли \n",
    "Все ключевые значения для расчётов сохраним в отдельных переменных.<br>\n",
    "Рассчитаем достаточный объём сырья для безубыточной разработки новой скважины. <br>\n",
    "Сравним полученный объём сырья со средним запасом в каждом регионе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевые значения для расчетов:\n",
    "\n",
    "TOTAL_DRILLS_NUMBER = 500 # При разведке региона исследуют 500 точек\n",
    "BEST_DRILLS_NUMBER = 200 # из которых с помощью машинного обучения выбирают 200 лучших для разработки\n",
    "BUDGET_FOR_REGION = 1e10  # Бюджет на разработку скважин в регионе — 10 млрд рублей.\n",
    "ONE_UNIT_PRICE = 450_000 #Доход с каждой единицы продукта составляет 450 тыс. рублей, поскольку объём указан в тысячах баррелей.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Совет:</b> Константы принято записывать в ВЕРХНЕМ РЕГИСТРЕ =) Читем здесь https://pythonworld.ru/osnovy/pep-8-rukovodstvo-po-napisaniyu-koda-na-python.html#section-29\n",
    "    \n",
    "А большие значение можно записывать так: `450_000`.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Константы записаны в верхнем регистре\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех[2]:</b> Так намного лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Достаточный объем сырья для окупаемости разработки скважин в регионе 111.111 тыс.баррелей с одной скважины\n"
     ]
    }
   ],
   "source": [
    "# Для безубыточной разработки скважины доход должен превышать расходы, т.е. (объем)х(доход с единицы)>(бюждет на разработку)\n",
    "# Объем безубыточности рассчитаем в тысячах баррелей, это бюджет поделенный на стоимость единицы\n",
    "\n",
    "# При этом вспомним, что по условию бюджет разработки дан на регион в целом, то есть его нужно еще поделить на кол-во скважин\n",
    "\n",
    "profitable_value = BUDGET_FOR_REGION / ONE_UNIT_PRICE / BEST_DRILLS_NUMBER\n",
    "\n",
    "print(f'Достаточный объем сырья для окупаемости разработки скважин в регионе {profitable_value:.3f} тыс.баррелей с одной скважины')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В регионе 0 скважина в среднем имеет дебит 92.424 тыс. баррелей\n",
      "В регионе 1 скважина в среднем имеет дебит 68.983 тыс. баррелей\n",
      "В регионе 2 скважина в среднем имеет дебит 95.116 тыс. баррелей\n",
      "При расчетном достаточном объеме для окупаемости разработки  111.111 тыс.баррелей с одной скважины\n"
     ]
    }
   ],
   "source": [
    "# Cопоставим этот объем с данными по скважинам. \n",
    "\n",
    "\n",
    "for i in range(len(regions)):\n",
    "    print(f'В регионе {i} скважина в среднем имеет дебит {zapas_pred[i]:.3f} тыс. баррелей')\n",
    "print(f'При расчетном достаточном объеме для окупаемости разработки  {profitable_value:.3f} тыс.баррелей с одной скважины')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:** <br><br>\n",
    "По предварительной оценке, если **случайным** образом пробурить 200 скважин, ни в одном регионе разработка не окупится, т.к. в среднем одна скважина не обеспечивает необходимый объем сырья. <br>\n",
    "Однако, если рассмотреть **лучшие по дебиту скважины**, ситуация может измениться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Значение для безубыточной разработки посчитано верно, с выводом согласен  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Шаг 4. Написание функции для расчета валовой прибыли \n",
    "Выберем скважины с максимальными значениями предсказаний.<br>\n",
    "Просуммируем целевое значение объёма сырья, соответствующее этим предсказаниям.<br>\n",
    "Рассчитаем валовую прибыль для полученного объёма сырья.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявляем функцию, которая на вход получит датасет с данными геологоразведки, \n",
    "# а также все необхожимые для расчета прогноза валовой прибыли данные\n",
    "\n",
    "# region_predict  - список предсказаных объемов\n",
    "# n - число лучших скважин, в которых будет осуществляться добыча\n",
    "# expenses - затраты на разработку в регионе\n",
    "# price - цена за единицу объема, указанного в region_predict\n",
    "# real_value - реальные данные о дебите скважим\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def oil_profit(region_predict, n, expenses, price, real_value):\n",
    "    \n",
    "      \n",
    "    # Сортируем предсказания, сохраняя индексы\n",
    "    value_sort = pd.Series(region_predict).sort_values(ascending=False)\n",
    "\n",
    "    # Определяем реальные показатели прибыли из реальных данных по индексам, суммируем их\n",
    "    region_n_best_value = real_value.iloc[value_sort.index][:n].sum()\n",
    "    \n",
    "    # выручка это произвежение общего объема на цену за единицу\n",
    "    income = region_n_best_value * price\n",
    "    \n",
    "    # валовая прибыль это доходы минус расходы\n",
    "    profit = income - expenses\n",
    "    \n",
    "    \n",
    "    return profit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Ф-ция для подсчета прибыли определена верно\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Регион 0 - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): 3.136 млрд. руб.\n",
      "Регион 1 - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): 2.415 млрд. руб.\n",
      "Регион 2 - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): 2.466 млрд. руб.\n"
     ]
    }
   ],
   "source": [
    "# Вызовем нашу функцию для расчета прибыли по трем регионам\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(regions)):\n",
    "\n",
    "    profit = oil_profit(predictions_valid[i], BEST_DRILLS_NUMBER, BUDGET_FOR_REGION, ONE_UNIT_PRICE, target_valid[i])\n",
    "    print(f'Регион {i} - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): {profit/1e9:.4} млрд. руб.')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**<br>\n",
    "Если располагать данными по 2500 разведочным скважинам (выборка valid), то из них вполне можно найти 200 лучших в каждом регионе, которые позволят получить валовую прибыль 2,5-3,8 млрд.рублей. Однако, если верить условиям задачи, в каждом регионе может быть разведано только 500 скважин, из которых и нужно будет определить лучшие 200. При таком \"раскладе\" шанс не получить прибыль сильно выше, т.к. мы видели выше, что в среднем случайная скважина не обеспечивает необходимый для прибыли дебит, а значит результат сильно чувствитетелен к тому, попадут ли в выборку из 500 скважин те лучшие, которые имеют хороший показатель прогнозных объемов и 200 которых смогут обеспечить прибыль.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# Шаг 5. Расчёт прибыли и рисков\n",
    "Применим технику Bootstrap с 1000 выборок, чтобы найти распределение прибыли.<br>\n",
    "Найдем среднюю прибыль, 95%-й доверительный интервал и риск убытков.<br>\n",
    "По итогам предложим регион для разработки скважин.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Регион 0\n",
      "Ожидаемая прибыль 0.426 млрд. руб., с вероятностью 95% в диапазоне от -0.098 до 0.954 млрд.руб.\n",
      "Вероятность (риск) получения убытка в регионе 5.7%\n",
      "\n",
      "Регион 1\n",
      "Ожидаемая прибыль 0.463 млрд. руб., с вероятностью 95% в диапазоне от 0.045 до 0.873 млрд.руб.\n",
      "Вероятность (риск) получения убытка в регионе 1.2%\n",
      "\n",
      "Регион 2\n",
      "Ожидаемая прибыль 0.328 млрд. руб., с вероятностью 95% в диапазоне от -0.157 до 0.846 млрд.руб.\n",
      "Вероятность (риск) получения убытка в регионе 10.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Проводим численный эксперимент:\n",
    "# 1. Из исходного датасета случайным образом формируем 1000 выборок по 500 строк, без возвращения (replace=False)\n",
    "#    используя модель Линейной регрессии (обученную на всей исходной выборке) прогнозируем объем по 500 скважинам\n",
    "# 2. На каждом шаге по 200 лучшим из 500-та выбранных скважин рассчитываем валовую прибыль\n",
    "# 3. Определяем среднюю прибыль c 200 лучших скважин и 95% доверительный интервал, риск убытков.\n",
    "\n",
    "# указываем random_state для воспроизводимости\n",
    "state = np.random.RandomState(42)\n",
    "\n",
    "# отдельно введем переменную с количеством итераций Bootstrap\n",
    "boot = 1000\n",
    "\n",
    "for j in range(len(regions)):\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for i in range(boot):\n",
    "        # извлекаем из выборки 500 (хранится в константе TOTAL_DRILLS_NUMBER) случайных строк\n",
    "        subsample_features = features_valid[j].sample(TOTAL_DRILLS_NUMBER, replace=True, random_state=state)\n",
    "        \n",
    "        # находим реальные значения дебита скважин для расчета прибыли\n",
    "        subsample_target = target_valid[j].iloc[subsample_features.index]\n",
    "        \n",
    "        # по выборке 500 скважин строим предсказание, используя ранее обученную модель и вызываем функцию расчета прибыли\n",
    "        values.append(oil_profit(model[j].predict(subsample_features), BEST_DRILLS_NUMBER, BUDGET_FOR_REGION, ONE_UNIT_PRICE, subsample_target))\n",
    "\n",
    "    # расчитываем границы 95% доверительного интервала (квантиль 2,5% снизу и 97,5% сверху)\n",
    "    values = pd.Series(values)\n",
    "    lower = values.quantile(q=0.025)\n",
    "    upper = values.quantile(q=0.975)    \n",
    "\n",
    "    # Расчет риска убытков. Вероятность получения убытков - это вероятность отрицательной прибыли.\n",
    "    # Рассчитаем количество отрицательных значений прибыли и поделим на количество экспериментов\n",
    "    neg_count = sum(1 for x in values if x < 0)\n",
    "    risk_prob = neg_count / boot *100\n",
    "    \n",
    "    \n",
    "    # выводим границы диапазона и среднее значение прибыли по регионам\n",
    "    print('Регион', j)\n",
    "    print(f'Ожидаемая прибыль {values.mean()/1e9:.3f} млрд. руб., с вероятностью 95% в диапазоне от {lower/1e9:.3f} до {upper/1e9:.3f} млрд.руб.')\n",
    "    print(f'Вероятность (риск) получения убытка в регионе {risk_prob:.1f}%')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Ошибка:</b>\n",
    "\n",
    "Особенностью техники бутстреп является семплирование с возвратом (`replace = True`, которое нужно явно указать), то есть наблюдение в выборку может попасть несколько раз. Это необходимо для того, чтобы добиться независимости при каждом выборе, иначе вероятность достать элемент из выборки будет с каждым разом увеличиваться. Причем чем больше размер подвыборки, тем выраженнее будет зависимость. Для подвыборки размера 1, например, в крайнем случае, мы возрат можем не делать, так уж и быть, а вот для подвыборки размером с семплируемую, без возврата ну никак не получится получить разнообразные варианты, а подвыборку большего размера вообще — не получить.\n",
    "    \n",
    "Все это может показаться немного странным для понимания: как будто одна скважина может буриться несколько раз. Но давай вспомним, что выборка - это лишь некоторое представление о генеральной совокупности. Вполне возможно, что где-то в регионе существуют и другие потенциальные точки добычи, о которых мы даже не подозреваем, но они являются частью генеральной сововкупности. В таком случае повторяющиеся наблюдения можно рассматривать как разные точки добычи из всей генеральной совокупности, просто они имеют близкие характеристики.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Были внесены следующие изменения\n",
    "    \n",
    "Применено семплирование с возвратом. Первоначально неверное понял смысл параметра replace: думал, что запрещая возврат (=False) мы запрещаем повторы наблюдений <b>в рамках одной случайно выборки</b>, то есть исключаем ситуацию, когда модель обучается на выборке, в которой несколько раз дублируется наблюдение. Спасибо за комментарий!\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<b>Ответ[2]:</b> `то есть исключаем ситуацию, когда модель обучается на выборке, в которой несколько раз дублируется наблюдение` - как раз таки, в рамках одной выбоки нам такое исключать и не нужно, второй абзац объясняет это простыми словами)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод**<br>\n",
    "Анализ методом Bootstrap показал, что если случайным образом исследовать 500 точек в регионе и выбрать 200 наилучших предсказаний по прибыльности, то в 95% вероятностью прибыль по региону будет лежать в указанных выше диапазонах, и в том числе может достигать отрицательных значений (то есть будет убыток). Веротяность получения убытков также рассчитан.<br>\n",
    "На основе проведенных расчетов предлагается остановить выбор на \"Регионе 1\", ожидаемая прибыль в котором составит 0,473 млрд.руб., при риске получения убытков 0,6%. Этот регион демонстрирует максимальную ожидаемую прибыль при минимальных рисках убытков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы по проекту\n",
    "\n",
    "В проекте был проведен анализ данных геологоразведки в трех регионах под условными номерами 0, 1 и 2.<br>\n",
    "\n",
    "1. Было предоставлено три идентичных набора данных, для удобства и оптимизации кода все данные были объеденены в один объект - словарь. Изменяя лишь переменную с номерами файлов можно масштабировать проект до любого адекватного количества данных о скважинах. \n",
    "\n",
    "Данные были загружены, проверены на полноту и сотоветствие типов, несущественные данные (id) удалены. Потребовалось провести масштабирование, которое было проведено после раздедения данных на выборки, во избежание эффекта утечки данных. <br>\n",
    "### UPD\n",
    "Проведена оценка корреляции признаков. По региону \"2\" всё отлично, корреляции минимальные. В регионе \"0\" есть заметная отрицательная корреляция признаков f0 и f1 (-0.44), а также признака f2 и целевого признака product (0.48). \n",
    "А вот с датасетов региона \"1\" ситуация сложнее. Высокая корреляция признака f2 и целевого признака product, значение практически единица. Это говорит о том, что имеется сильно выраженная прямопропорциональная зависимость между признаком f2 и целевым признаком. **Но по условиям проекта - все признаки значимы и удалять мы их не будем.**\n",
    "\n",
    "\n",
    "\n",
    "2. Данные разделены на обучающую и валидационную выборки в соотношении 75:25.<br>\n",
    "Обучение моделей проведено отдельно для каждого набора данных по регионам геологоразведки на обучаюзщих выборках, предсказания выполнены на валидационных выборках.<br>\n",
    "Рассчитан средний запас предсказанного сырья и RMSE моделей:<br>\n",
    "\n",
    "Регион 0: Средний запас сырья на скважину 92.42384/предс/, 92.39349/реал/, RMSE 37.71690 (тыс.баррелей)<br>\n",
    "Регион 1: Средний запас сырья на скважину 68.98312/предс/, 68.98037/реал/, RMSE 0.89149 (тыс.баррелей)<br>\n",
    "Регион 2: Средний запас сырья на скважину 95.11622/предс/, 94.54782/реал/, RMSE 39.97554 (тыс.баррелей)<br>\n",
    "\n",
    "Метрика **RMSE** (средневкадратичное отклонение) исслюстрирует насколько хорошо модель соответствует набору данных. Чем больше RMSE, тем больше разница между прогнозируемыми и наблюдаемыми значениями, а это означает, что модель хуже соответствует данным. И наоборот, чем меньше RMSE, тем лучше модель соответствует данным. Таким образом мы видим, что в нашем проекте наилучшее предсказание получилось по данным региона 1, значение RMSE минимальное.  По регионам 0 и 2 среднеквадративное отклонение значительно выше, хотя средние значения предсказанных запасов сырья в целом близки к реальным данным.<br>\n",
    "\n",
    "3. Проведена подготовка к расчёту прибыли. <br>\n",
    "Все ключевые значения для расчётов сохранены в отдельных переменных.<br>\n",
    "Рассчитан достаточный объём сырья для безубыточной разработки новой скважины, который составил 111.111 тыс.баррелей с одной скважины.<br>\n",
    "Проведено сравнение полученного объёма сырья со средним запасом в каждом регионе: <br>\n",
    "В регионе 0 скважина в среднем имеет дебит 92.424 тыс. баррелей<br>\n",
    "В регионе 1 скважина в среднем имеет дебит 68.983 тыс. баррелей<br>\n",
    "В регионе 2 скважина в среднем имеет дебит 95.116 тыс. баррелей<br>\n",
    "\n",
    "Таким образом, по предварительной оценке, если бурить скважины **случайным** образом, ни в одном регионе разработка не окупится, т.к. в среднем одна скважина не обеспечивает необходимый объем сырья. <br>\n",
    "\n",
    "Однако, если рассмотреть **лучшие по дебиту скважины**, ситуация может измениться.\n",
    "\n",
    "\n",
    "4. Написана функции для расчета валовой прибыли. Для этого выбраны скважины с максимальными значениями предсказаний. Просуммированы целевые значения объёма сырья, соответствующие этим предсказаниям. Рассчитана валовая прибыль для полученного объёма сырья:<br>\n",
    "Регион 0 - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): 3.136 млрд. руб.<br>\n",
    "Регион 1 - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): 2.415 млрд. руб.<br>\n",
    "Регион 2 - прогноз прибыли с 200 лучших скважин (по предсказанным объемам): 2.466 млрд. руб.<br>\n",
    "\n",
    "Если располагать данными по 2500 разведочным скважинам (выборка valid), то из них вполне можно найти 200 лучших в каждом регионе, которые позволят получить валовую прибыль 2,5-3,8 млрд.рублей. Однако, если верить условиям задачи, в каждом регионе может быть разведано только 500 скважин, из которых и нужно  определить лучшие 200. При таком \"раскладе\" шанс не получить прибыль сильно выше, т.к. мы видели выше, что в среднем случайная скважина не обеспечивает необходимый для прибыли дебит, а значит результат сильно чувствитетелен к тому, попадут ли в выборку из 500 скважин те лучшие, которые имеют хороший показатель прогнозных объемов и 200 которых смогут обеспечить прибыль.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "5. Проведен расчёт прибыли и рисков\n",
    "Была применена технику Bootstrap с 1000 выборок, определено распределение прибыли.<br>\n",
    "Найдена средняя прибыль, 95%-й доверительный интервал и риск убытков.<br>\n",
    "По итогам предложен регион для разработки скважин.\n",
    "\n",
    "Анализ методом Bootstrap показал, что если случайным образом исследовать 500 точек в регионе и выбрать 200 наилучших предсказаний по прибыльности, то в 95% вероятностью прибыль по региону будет лежать в указанных выше диапазонах, и в том числе может достигать отрицательных значений (то есть будет убыток). Веротяность получения убытков также рассчитан.<br>\n",
    "\n",
    "### UPD\n",
    "### На основе проведенных расчетов предлагается остановить выбор на \"Регионе 1\", ожидаемая прибыль в котором составит 0,436 млрд.руб., при риске получения убытков 1,2%. Этот регион демонстрирует максимальную ожидаемую прибыль при минимальных рисках убытков.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Не забудь обновить вывод, если потребуется.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Изменения:</b> Выводы обновлены (помечено UPD)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех[2]:</b> Отлично, проведено детальное исследование и был выбран правильный регион!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Если интересно изучить тему бутстрапа глубже - я оставлю  ниже несколько интересных и полезных ссылок по теме:\n",
    "        \n",
    "1. \"Bootstrap Hypothesis Testing in Statistics with Example\" \n",
    "        \n",
    "https://www.youtube.com/watch?v=9STZ7MxkNVg\n",
    "        \n",
    "2. \"How to Calculate Bootstrap Confidence Intervals For Machine Learning Results in Python\" \n",
    "        \n",
    "https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "        \n",
    "3. \"A Gentle Introduction to the Bootstrap Method\" \n",
    "\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-the-bootstrap-method/\n",
    "        \n",
    "4. \"An Introduction to the Bootstrap Method\" \n",
    "        \n",
    "https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60#:~:text=The%20basic%20idea%20of%20bootstrap,population%20mean)%20on%20sample%20data.&amp;text=A%20sample%20from%20population%20with%20sample%20size%20n\n",
    "        \n",
    "5. \"Bootstrapping and Resampling in Statistics with Example\" \n",
    "        \n",
    "        \n",
    "https://www.youtube.com/watch?v=O_Fj4q8lgmc\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 427,
    "start_time": "2022-12-15T08:44:35.692Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
